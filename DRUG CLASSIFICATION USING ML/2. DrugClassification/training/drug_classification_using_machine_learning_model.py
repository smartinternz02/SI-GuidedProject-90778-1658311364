# -*- coding: utf-8 -*-
"""Drug Classification using Machine Learning Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19QWPQJ7sF0lezzSSK8WDgejiiqwAf_bE

<style>
h1{
  font-size: 20px;
}
</style>

<center><h1>DRUG CLASSIFICATION USING MACHINE LEARNING</h1></center>
<br>
<center><h2>CONTENTS</h2></center>

1. Introduction 
2.Importing Libraries
3. Reading Data Set 
4. Initial Dataset Exploration 

 4.1 Categorical Variables 

 4.2 Descriptive Analysis 

5. Univariate Analysis

 5.1 Drug Type Distribution

 5.2 Gender Distribution 

 5.3 Blood Pressure Distribution 

 5.4 Cholesterol Distribution

 5.5 Gender Distribution based on Drug Type 

 5.6 Blood Pressure Distribution based on Cholesetrol 

 5.7 Sodium to Potassium Distribution based on Gender and Age
6.Bivariate analysis
7.Multivariate Analysis
8.Encoding- Converting categorical to numerical
9.Data Preparation

 9.1 Data Bining

      9.1.1 Age

      9.1.2 Na_to_K 
   9.2 Splitting the dataset 

   9.3 Feature Engineering

10. Model Building 

   10.1 Logistic Regression

   10.2 K-nearest Neighbours

   10.3 Naive Bayes Classifier

   10.4 Decision Tree Making

   10.5 Random Forest Classifier

11. Model Evaluation and comparison
12. Saving the model
13. Conclusion

# 1. Introduction 
<center><img src="https://cdn-aimkf.nitrocdn.com/fHzWlVYOXDdxWdFNbwxEdhdjxmnEjSJH/assets/static/optimized/rev-dadfeca/wp-content/uploads/2019/03/drug-classification-charts.webp" alt="Drug Picture" width="1000" height="600"></center><br>

This dataset contains information about drug classification based on patient general information and its diagnosis. Machine learning model is needed in order **to predict the outcome of the drugs type** that might be suitable for the patient.


**The machine learning models used in this project are:** 
1. Linear Logistic Regression
3. K-nearest Neighbours
4. Naive Bayes Classifier
5. Decision Tree Making
6. Random Forest Classifier

---

## Data Set Description ðŸ§¾

 There are **6 variables** in this data set:
*   **4 categorical** variables.
*   **2 continuous** variables.

<br>

<i>The following is the **structure of the data set**.</i>


<table style="width:100%">
<thead>
<tr>
<th style="text-align:center; font-weight: bold; font-size:14px">Variable Name</th>
<th style="text-align:center; font-weight: bold; font-size:14px">Description</th>
<th style="text-align:center; font-weight: bold; font-size:14px">Sample Data</th>
</tr>
</thead>
<tbody>
<tr>
<td><b>Age</b></td>
<td>Patient Age</td>
<td>23; 47; ...</td>
</tr>
<tr>
<td><b>Sex</b></td>
<td>Gender of patient <br> (male or female)</td>
<td>F; M; ...</td>
</tr>
<tr>
<td><b>BP</b></td>
<td>Levels of blood pressure <br> (high, normal, or low)</td>
<td>HIGH; NORMAL; LOW; ...</td>
</tr>
<tr>
<td><b>Cholesterol</b></td>
<td>Levels of cholesterol <br> (high or normal)</td>
<td>1.4; 1.3; ...</td>
</tr>
<tr>
<td><b>Na_to_K</b></td>
<td>Sodium to potassium ratio in blood</td>
<td>25.355; 13.093; ...</td>
</tr>
<tr>
<td><b>Drug</b></td>
<td>Type of drug</td>
<td>DrugY; drugC; ...</td>
</tr>
</tbody>
</table>

# 2. Importing Libraries ðŸ“š
**Importing libraries** that will be used in this notebook.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import warnings
import pickle
from scipy import stats
warnings.filterwarnings('ignore')
plt.style.use('fivethirtyeight')

"""# 3. Reading Data Set 
ðŸ‘‰ After importing libraries, we will also **import the dataset** that will be used.
"""

df = pd.read_csv("drug200.csv")

"""ðŸ‘‰ Read the first 6 rows in the dataset."""

df.head()

"""Data type and checking null in dataset."""

print(df.info())

"""<i>From the results above, **there are no missing/null value** in this dataset.</i>

# 4. Initial Dataset Exploration 
<i>This section will explore raw dataset that has been imported.</i>

## 4.1 Categorical Variables
"""

df.Drug.value_counts()

"""<i>It can be seen that from results above, DrugY has more amount than other types of drugs.</i>"""

df.Sex.value_counts()

"""<i>The distribution of patient gender is balanced.</i>"""

df.BP.value_counts()

"""<i> The distribution of blood pressure level is balanced.</i>"""

df.Cholesterol.value_counts()

"""The distribution of cholesterol level is balanced.

## 4.2 Descriptive Analysis 
This section will show the dataset information using describe function. The skewness value for each numerical variables will also shown in this section.
"""

df.describe(include="all")

skewAge = df.Age.skew(axis = 0, skipna = True)
print('Age skewness: ', skewAge)

skewNatoK = df.Na_to_K.skew(axis = 0, skipna = True)
print('Na to K skewness: ', skewNatoK)

sns.distplot(df['Age']);

sns.distplot(df['Na_to_K']);

"""ðŸ‘‰The distribution of **'Age'** column is **symetric**, since the skewness value  between -0.5 and 0.5 <br>
ðŸ‘‰The distribution of **'Na_to_K'** column is **moderately skewed**, since the skewness value is ***between 0.5 and 1***. It can also be seen from the histogram for 'Na_to_K' column

# 5. Univariate Analysis
This section will explore variables in the dataset using different various plots/charts.

## 5.1 Drug Type Distribution ðŸ’Š
"""

sns.set_theme(style="darkgrid")
sns.countplot(y="Drug", data=df, palette="flare")
plt.ylabel('Drug Type')
plt.xlabel('Total')
plt.show()

"""## 5.2 Gender Distribution ðŸ‘«"""

sns.set_theme(style="darkgrid")
sns.countplot(x="Sex", data=df, palette="rocket")
plt.xlabel('Gender (F=Female, M=Male)')
plt.ylabel('Total')
plt.show()

"""## 5.3 Blood Pressure Distribution ðŸ©¸"""

sns.set_theme(style="darkgrid")
sns.countplot(y="BP", data=df, palette="crest")
plt.ylabel('Blood Pressure')
plt.xlabel('Total')
plt.show()

"""## 5.4 Cholesterol Distribution ðŸ¥›"""

sns.set_theme(style="darkgrid")
sns.countplot(x="Cholesterol", data=df, palette="magma")
plt.xlabel('Blood Pressure')
plt.ylabel('Total')
plt.show()

"""## 5.5 Gender Distribution based on Drug Type ðŸ‘«ðŸ’Š"""

pd.crosstab(df.Sex,df.Drug).plot(kind="bar",figsize=(12,5),color=['#003f5c','#ffa600','#58508d','#bc5090','#ff6361'])
plt.title('Gender distribution based on Drug type')
plt.xlabel('Gender')
plt.xticks(rotation=0)
plt.ylabel('Frequency')
plt.show()

"""## 5.6 Blood Pressure Distribution based on Cholesetrol ðŸ©¸ðŸ¥›"""

pd.crosstab(df.BP,df.Cholesterol).plot(kind="bar",figsize=(15,6),color=['#6929c4','#1192e8'])
plt.title('Blood Pressure distribution based on Cholesterol')
plt.xlabel('Blood Pressure')
plt.xticks(rotation=0)
plt.ylabel('Frequency')
plt.show()

"""## 5.7 Sodium to Potassium Distribution based on Gender and Age ðŸ§ªðŸ‘«ðŸ‘´"""

plt.scatter(x=df.Age[df.Sex=='F'], y=df.Na_to_K[(df.Sex=='F')], c="Blue")
plt.scatter(x=df.Age[df.Sex=='M'], y=df.Na_to_K[(df.Sex=='M')], c="Orange")
plt.legend(["Female", "Male"])
plt.xlabel("Age")
plt.ylabel("Na_to_K")
plt.show()

"""# 6.Bivariate analysis"""

df.head()

plt.figure(figsize=(20,5))
plt.subplot(131)
sns.countplot(df['Drug'],hue=df['BP'])
plt.legend(loc='upper right')
plt.subplot(132)
sns.countplot(df['Drug'],hue=df['Sex'])
plt.subplot(133)
sns.countplot(df['Drug'],hue=df['Cholesterol'])

# Creating a new column Age_. This column shows the categorized age.

df['Age_'] = ['15-30' if x<=30 else '30-50' if x>30 and x<=50 else '50-75' for x in df['Age']]
df.head()

# Finding the relation between categorized age and drug

pd.crosstab(df['Age_'],[df['Drug']])

# Removing the Age_ column

df.drop('Age_',axis=1,inplace=True)
df.head()

"""# 7.Multivariate Analysis"""

sns.swarmplot(df['Drug'],df['Na_to_K'],hue=df['BP'])

# DrugC is used for low BP patient, DrugY is used on patients having Na_to_K > 15.

"""# 8.Encoding- Converting categorical to numerical"""

# Replacing low, normal & high with 0, 1 & 2...

df['BP'] = [0 if x=='LOW' else 1 if x=='NORMAL' else 2 for x in df['BP']]

# Replacing normal and high cholesterol with 0 & 1

df['Cholesterol'] = [0 if x=='NORMAL' else 1 for x in df['Cholesterol']]

# Replacing female and male with 0 & 1

df['Sex'] = [0 if x=='F' else 1 for x in df['Sex']]

df.head()

df['Drug'].value_counts()

x = df.drop('Drug',axis=1)
x.head()

y = df['Drug']
y.head()

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=10)

print('Shape of x_train {}'.format(x_train.shape))
print('Shape of y_train {}'.format(y_train.shape))
print('Shape of x_test {}'.format(x_test.shape))
print('Shape of y_test {}'.format(y_test.shape))

"""#9. Data Preparation âš™
ðŸ‘‰ This section will prepare the dataset before building the machine learning models.

## 9.1 Data BiningðŸš®

### 9.1.1 Age ðŸ‘´
ðŸ‘‰ The age will be divided into **7 age categories**:
*  Below 20 y.o.
*  20 - 29 y.o.
*  30 - 39 y.o.
*  40 - 49 y.o.
*  50 - 59 y.o.
*  60 - 69 y.o.
*  Above 70.
"""

bin_age = [0, 19, 29, 39, 49, 59, 69, 80]
category_age = ['<20s', '20s', '30s', '40s', '50s', '60s', '>60s']
df['Age_binned'] = pd.cut(df['Age'], bins=bin_age, labels=category_age)
df = df.drop(['Age'], axis = 1)

"""### 9.1.2 Na_to_K ðŸ§ª
ðŸ‘‰ The chemical ratio will be divided into **4 categories**:
*  Below 10.
*  10 - 20.
*  20 - 30.
*  Above 30.
"""

bin_NatoK = [0, 9, 19, 29, 50]
category_NatoK = ['<10', '10-20', '20-30', '>30']
df['Na_to_K_binned'] = pd.cut(df['Na_to_K'], bins=bin_NatoK, labels=category_NatoK)
df = df.drop(['Na_to_K'], axis = 1)

"""## 9.2 Splitting the dataset ðŸª“
ðŸ‘‰ The dataset will be split into **70% training and 30% testing**.
"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

X = df.drop(["Drug"], axis=1)
y = df["Drug"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

"""## 9.3 Feature Engineering ðŸ”§
ðŸ‘‰ The FE method that used is **one-hot encoding**, which is **transforming categorical variables into a form that could be provided to ML algorithms to do a better prediction**.
"""

X_train = pd.get_dummies(X_train)
X_test = pd.get_dummies(X_test)

X_train.head()

X_test.head()

"""ðŸ‘‰ As can be seen, the distrubtion of drug type are now balanced.

# 10. Model Building

## 10.1 Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
LRclassifier = LogisticRegression(solver='liblinear', max_iter=5000)
LRclassifier.fit(X_train, y_train)

y_pred = LRclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
LRAcc = accuracy_score(y_pred,y_test)
print('Logistic Regression accuracy is: {:.2f}%'.format(LRAcc*100))

"""## 10.2 K-Nearest Neighbours"""

from sklearn.neighbors import KNeighborsClassifier
KNclassifier = KNeighborsClassifier(n_neighbors=20)
KNclassifier.fit(X_train, y_train)

y_pred = KNclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
KNAcc = accuracy_score(y_pred,y_test)
print('K Neighbours accuracy is: {:.2f}%'.format(KNAcc*100))

scoreListknn = []
for i in range(1,30):
    KNclassifier = KNeighborsClassifier(n_neighbors = i)
    KNclassifier.fit(X_train, y_train)
    scoreListknn.append(KNclassifier.score(X_test, y_test))
    
plt.plot(range(1,30), scoreListknn)
plt.xticks(np.arange(1,30,1))
plt.xlabel("K value")
plt.ylabel("Score")
plt.show()
KNAccMax = max(scoreListknn)
print("KNN Acc Max {:.2f}%".format(KNAccMax*100))

"""## 10.3 Naive Bayes Classifier

"""

from sklearn.naive_bayes import CategoricalNB
NBclassifier1 = CategoricalNB()
NBclassifier1.fit(X_train, y_train)

y_pred = NBclassifier1.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
NBAcc1 = accuracy_score(y_pred,y_test)
print('Naive Bayes accuracy is: {:.2f}%'.format(NBAcc1*100))

"""## 10.4 Decision Tree Making"""

from sklearn.tree import DecisionTreeClassifier
DTclassifier = DecisionTreeClassifier(max_leaf_nodes=20)
DTclassifier.fit(X_train, y_train)

y_pred = DTclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
DTAcc = accuracy_score(y_pred,y_test)
print('Decision Tree accuracy is: {:.2f}%'.format(DTAcc*100))

scoreListDT = []
for i in range(2,50):
    DTclassifier = DecisionTreeClassifier(max_leaf_nodes=i)
    DTclassifier.fit(X_train, y_train)
    scoreListDT.append(DTclassifier.score(X_test, y_test))
    
plt.plot(range(2,50), scoreListDT)
plt.xticks(np.arange(2,50,5))
plt.xlabel("Leaf")
plt.ylabel("Score")
plt.show()
DTAccMax = max(scoreListDT)
print("DT Acc Max {:.2f}%".format(DTAccMax*100))

"""## 10.5 Random Forest Classifier




"""

from sklearn.ensemble import RandomForestClassifier

RFclassifier = RandomForestClassifier(max_leaf_nodes=30)
RFclassifier.fit(X_train, y_train)

y_pred = RFclassifier.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

from sklearn.metrics import accuracy_score
RFAcc = accuracy_score(y_pred,y_test)
print('Random Forest accuracy is: {:.2f}%'.format(RFAcc*100))

scoreListRF = []
for i in range(2,50):
    RFclassifier = RandomForestClassifier(n_estimators = 1000, random_state = 1, max_leaf_nodes=i)
    RFclassifier.fit(X_train, y_train)
    scoreListRF.append(RFclassifier.score(X_test, y_test))
    
plt.plot(range(2,50), scoreListRF)
plt.xticks(np.arange(2,50,5))
plt.xlabel("RF Value")
plt.ylabel("Score")
plt.show()
RFAccMax = max(scoreListRF)
print("RF Acc Max {:.2f}%".format(RFAccMax*100))

rf = RandomForestClassifier()
rf.fit(x_train,y_train)

ypred = rf.predict(x_test)

confusion_matrix(y_test,ypred)

print(classification_report(y_test,ypred))

"""# 11. Model Evaluation and comparison ðŸ‘€"""

compare = pd.DataFrame({'Model': ['Logistic Regression', 'K Neighbors', 'K Neighbors Max', 'SVM', 'Categorical NB', 'Gaussian NB', 'Decision Tree', 'Decision Tree Max', 'Random Forest', 'Random Forest Max'], 
                        'Accuracy': [LRAcc*100, KNAcc*100, KNAccMax*100, SVCAcc*100, NBAcc1*100, NBAcc2*100, DTAcc*100, DTAccMax*100, RFAcc*100, RFAccMax*100]})
compare.sort_values(by='Accuracy', ascending=False)

"""ðŸ‘‰ From the results, it can be seen that most of ML models can reach **up to 80% accuracy** in predicting classification of drug type.

# 12. Saving the model
ðŸ‘‰ The next step is to save the model in pickle format.
"""

pickle.dump(rf,open('model.pkl','wb'))